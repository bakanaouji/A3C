\documentclass{jarticle}
\usepackage{fancybox}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage[dvipdfmx]{graphicx}
\usepackage{wrapfig}%文字の画像回り込み
\usepackage{comment}
\usepackage{multicol}
\usepackage{txfonts,here}
\usepackage{algorithm} %algorithmとalgorithmic環境を利用するのに必要．
\usepackage{algcompatible}
\algblockdefx{FORP}{ENDFORP}[1]%
  {\textbf{for}#1 \textbf{do in parallel}}%
  {\textbf{end for}}
\usepackage{bm}
\usepackage{longtable}
\usepackage{multirow}
\usepackage{colortbl}
\usepackage{subfigure}
\usepackage{listings}
\usepackage[top=30truemm,bottom=30truemm,left=25truemm,right=25truemm]{geometry}
\usepackage{url}
% \newcommand{\argmax}{\mathop{\rm arg~max}\limits}
% \newcommand{\argmin}{\mathop{\rm arg~min}\limits}
%%%% ↓algotithmic の \REQUIRE と \ENSURE の表記を変更する
\renewcommand{\algorithmicrequire}{\textbf{Input:}}
\renewcommand{\algorithmicensure}{\textbf{Output:}}
%%%% ↑algotithmic の \REQUIRE と \ENSURE の表記を変更する

\begin{document}

\section{A3Cの擬似コード}
\begin{algorithm}[htb]
\caption{各スレッド毎のA3Cの擬似コード}
\label{alg:a3c}                          
\begin{algorithmic}[1]   
\REQUIRE 
\STATE //グローバルパラメータ$\theta$と$\theta_v$，グローバルカウンター$T=0$は全スレッドで共有する
\STATE // パラメータ$\theta^{\prime}$と$\theta_v^{\prime}$は各スレッド毎に保有する
\STATE $t\gets 1$
\WHILE{$T<T_{\mathrm{max}}$}
	\STATE $d\theta\gets 0,~d\theta_v\gets 0$
	\STATE $\theta^{\prime}=\theta,~\theta_v^{\prime}=\theta_v$
	\STATE $t_{\mathrm{start}}=t$
	\STATE $s_t,~done_t$を観測
	\STATE $done=done_t$
	\WHILE{$!done$ and $t-t_{\mathrm{start}}<t_{\mathrm{max}}$}
		\STATE $\pi(a_t|s_t;\theta^{\prime})$に従って$a_t$を実行
		\STATE $(s_{t+1},~r_t,~done_t)$を観測
		\STATE $done=done_t$
		\STATE $t\gets t+1$
		\STATE $T\gets T+1$
	\ENDWHILE
	\IF{$done$}
	\STATE $R=0$
	\ELSE
	\STATE $R=V(s_t,~\theta_v^{\prime})$~~~~// 最終状態からのブートストラップ
	\ENDIF
	\FOR{$i\in\{t-1,~\cdots,~t_{\mathrm{start}}\}$}
		\STATE $R\gets r_i+\gamma R$
		\STATE $d\theta\gets d\theta+\Delta_{\theta^{\prime}}\log{\pi(a_i|s_i;\theta^{\prime})(R-V(s_i;\theta_v^{\prime}))}$~~~~// $\theta^{\prime}$に関する勾配を蓄積
		\STATE $d\theta_v\gets d\theta_v+\delta(R-V(s_i;\theta_v^{\prime}))^2/\delta\theta_v^{\prime}$~~~~// $\theta_v^{\prime}$に関する勾配を蓄積
	\ENDFOR
	\STATE $d\theta,~d\theta_v$を使って$\theta,~\theta_v$を非同期更新
\ENDWHILE
\ENSURE
\end{algorithmic}
\end{algorithm}


\section{Atari Gamesの環境のラップ処理}
Deep Q NetworkではAtari Gamesから受け取った状態の前処理などを行うことで学習をしやすくしている．
以下にそれらの処理を示す．
なお，$reset_{\mathrm{hoge}}()$は環境をリセットするときに呼ぶメソッドを，$step_{\mathrm{hoge}}(a_t)$は行動$a_t$を取って環境を更新するときに呼ぶメソッドを，$observe_{\mathrm{hoge}}()$は環境の状態を返すときに呼ばれるメソッドを，$reward_{\mathrm{hoge}}()$は報酬を返すときに呼ばれるメソッドを表す．
\begin{algorithm}[htb]
\caption{$reset_{\mathrm{noop}}()$}
\label{alg:noop_reset}                          
\begin{algorithmic}[1]   
\REQUIRE $env, l_{\mathrm{nomax}}$
\STATE エピソードの開始時に，数フレーム何もしない行動を取り，初期状態を決定する．
\STATE $T\sim U(1,l_{\mathrm{nomax}})$
\FOR{$t^{\prime}=1,\cdots,T$}
\STATE $a_{t^{\prime}}=(\mbox{do nothing})$の実行
\ENDFOR
\ENSURE 初期状態が決定した環境$env$
\end{algorithmic}
\end{algorithm}

\begin{algorithm}[tb]
\caption{$step_{\mathrm{repeat}}(a_t)$}
\label{alg:max_and_skip}                          
\begin{algorithmic}[1]   
\REQUIRE $env, l_{\mathrm{repeat}}, a_t$
\STATE 1回行動を取ると，同じ行動を指定フレーム続ける．指定数分行動を繰り返したら，直前のフレームの観測との最大値を状態として返す．
\STATE ※$a_t$は選択したい行動とする．
\STATE $r_{\mathrm{total}}=0$
\FOR{$t^{\prime}=1,\cdots,l_{\mathrm{repeat}}$}
\STATE $s_{\mathrm{prev}}=s_{t^{\prime}}$
\STATE $a_{t^{\prime}}=a_t$として行動を選択し，環境$env$を更新，$(s_{t^{\prime}+1},r_{t^{\prime}}, done_{t^{\prime}})$を観測する．
\STATE $r_{\mathrm{total}}=r_{\mathrm{total}}+r_{t^{\prime}}$
\STATE $s_{\mathrm{max}}=\max(s_{\mathrm{prev}}, s_{t^{\prime}+1})$
\STATE $done=done_{t^{\prime}}$
\ENDFOR
\ENSURE $s_{\mathrm{max}}, r_{\mathrm{total}}, done$
\end{algorithmic}
\end{algorithm}

\begin{algorithm}[tb]
\caption{$observe_{\mathrm{gray84}}()$}
\label{alg:process_frame84}                          
\begin{algorithmic}[1]   
\REQUIRE $s_t$
\STATE 観測した画面を(84,84)サイズのグレースケール画像に変換して返す．
\STATE $s_t$をグレースケール画像に変換
\STATE 変換後の$s_t$をさらに(84,84)にreshape
\ENSURE 変換後の$s_t$
\end{algorithmic}
\end{algorithm}

\begin{algorithm}[tb]
\caption{$step_{\mathrm{stack}}(a_t)$}
\label{alg:frame_stack}                          
\begin{algorithmic}[1]   
\REQUIRE $env, l_{\mathrm{history}}, a_t, S$
\STATE $l_{\mathrm{history}}$ステップ数分の観測の履歴を状態として返す．
\STATE $S$は観測の履歴とする．
\STATE $a_t$を行動として選択し，環境$env$を更新，$(s_{t+1},r_t, done_t)$を観測する．
\STATE $S$に$s_{t+1}$を追加．$|S|>l_{\mathrm{history}}$なら，一番古い履歴を$S$から削除する．
\ENSURE $S, r_t, done_t$
\end{algorithmic}
\end{algorithm}

\begin{algorithm}[tb]
\caption{$reward_{\mathrm{clip}}()$}
\label{alg:clipped_reward_wrapper}                          
\begin{algorithmic}[1]   
\REQUIRE $r_t$
\STATE 報酬$r_t$が正なら$+1$に，負なら$-1$に，$0$なら$0$として返す．
\IF{$r_t>0$}
\STATE $r_t=1$
\ELSIF{$r_t<0$}
\STATE $r_t=-1$
\ELSE
\STATE $r_t=0$
\ENDIF
\ENSURE $r_t$
\end{algorithmic}
\end{algorithm}

\end{document}
