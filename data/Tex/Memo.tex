\documentclass{jarticle}
\begin{document}

\section{論文メモ書き}

\subsection{Momentum SGD}
非同期なSGDの実装は"A lock-free approach to paralleliz- ing stochastic gradient descent"などで検討されていて，実装が簡単．

$\theta$を全てのスレッド間で共有するパラメータベクトル，$\Delta \theta_i$をi番目のスレッドによって計算された$\theta$の勾配とする．

各スレッドiはモメンタム項の更新$m_i=\alpha m_i+(1-\alpha)\Delta\theta_i$とパラメータの更新
$$\theta\gets\theta-\eta m_i$$
をロックなしで独立して行う．

ここで，各スレッドは独自の勾配とモメンタムベクトルを保持している．

\subsection{RMSProp}
非同期なRMSPropに関する研究はあまりされていない．

標準的なRMSPropの更新式は，以下で与えられる．
$$g=\alpha g + (1-\alpha)\Delta\theta^2$$
$$\theta \gets \theta - \eta \frac{\Delta\theta}{\sqrt{g+\epsilon}}$$

非同期でRMSProsを適用するには，要素ごとの$g$の移動平均をスレッドごとに共有するかどうかを決定する必要がある．

ここで，２つのパターンで実験する．
１つは，各スレッド毎に$g$を保持する．
もう１つはShared RMSPropと呼ばれ，ベクトル$g$はスレッド間で共有され，非同期に更新される．
スレッド間で共有することで，メモリを削減できる．

\subsection{実験設定}
\subsubsection{共通設定}
\begin{itemize}
	\item スレッド数 : 16
	\item 5回行動するごとにパラメータを更新
	\item shared target network は40,000フレームごとに更新
	\item Atariの実験では，"Human-level control through deep reinforcement learning"と同じ入力の前処理を用いる
	\item 行動繰り返し数 : 4
	\item ネットワークの構造は"Playing atari with deep reinforce- ment learning"と同じものを用いる（ストライド4の8×8のフィルターを16個有する畳み込み層，ストライド2の4×4のフィルターを32個有する畳み込み層，256個のユニットを持つ全結合層から成る．各層の活性化関数はReLU．）
	\item 割引率$\gamma$ : $0.99$
	\item 減衰係数$\alpha$ : $0.99$
	\item 初期学習率は$LogUniform(10^{-4},10^{-2})$からサンプルし，学習中に$0.0$に線形減少させる．
\end{itemize}

\subsubsection{価値関数法}
\begin{itemize}
	\item ネットワークの出力 : 状態-行動の価値
	\item $\epsilon$ : ３つの値$\epsilon_1,\epsilon_2,\epsilon_3$からそれぞれ確率$0.4,0.3,0.3$で確率的に決定
	\item $\epsilon_1,\epsilon_2,\epsilon_3$は，はじめの400万フレームに渡って$1.0$からそれぞれ$0.1,0.01,0.5$に線形減少させる．
\end{itemize}

\subsubsection{A3C}
\begin{itemize}
	\item ネットワークの出力 : 各行動を選択する確率（softmax）と，状態の価値
	\item エントロピー正則化の強さ : $\beta=0.01$
\end{itemize}

\end{document}